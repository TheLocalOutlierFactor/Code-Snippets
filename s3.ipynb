{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cd1360-45dd-4baf-800f-1499d35f4217",
   "metadata": {},
   "source": [
    "# Functions to work with s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7234136-13eb-47cd-8c07-e0abb4eb202c",
   "metadata": {},
   "source": [
    "## Working with s3 using AWS cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6426b4-6073-41b1-a5f9-f4338fda3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to work with s3 via AWS cli.\n",
    "\n",
    "Can be run in command line and is relatively fast.\n",
    "'''\n",
    "\n",
    "# installing aws-cli to local machine\n",
    "!curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "!unzip awscliv2.zip\n",
    "!./aws/install -i /usr/local/aws-cli -b /usr/local/bin\n",
    "\n",
    "\n",
    "# loading up credentials and configuring aws cli\n",
    "with open('secrets.json', 'r') as j:\n",
    "     secrets = json.load(j)\n",
    "key = secrets['aws_access_key_id']\n",
    "secret = secrets['aws_secret_access_key']\n",
    "\n",
    "!aws configure set aws_access_key_id $key\n",
    "!aws configure set aws_secret_access_key $secret\n",
    "!aws configure set region ru-1 \n",
    "\n",
    "\n",
    "# syncing local folder with s3 folder\n",
    "# works both for uploading and downloading\n",
    "# fast, but not as fast as multiprocessing + multithreading\n",
    "bucket_path = 's3://test-bucket/'\n",
    "s3_path = bucket_path + 'test/images'\n",
    "\n",
    "local_path = './test/images'\n",
    "\n",
    "!aws s3 sync $s3_path $local_path --only-show-errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a5265-cfcc-4443-965c-9f346fc451a3",
   "metadata": {},
   "source": [
    "## Working with s3 using Python + Boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3aed6-212c-4fba-8d21-8e3deea4d085",
   "metadata": {},
   "source": [
    "### Setting up connection with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0133a2e-6ca1-4d4a-9471-cd9951d86174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "def setup_s3_connection(secrets_path='secrets.json', service_name='s3', endpoint_url=None, region_name=None):\n",
    "    '''\n",
    "    Create Boto3 client object for s3 access.\n",
    "\n",
    "    Requires json file with s3 credentials in following format:\n",
    "    {\n",
    "        aws_access_key_id: \"qwerty\"\n",
    "        aws_secret_access_key: \"a1b2c3d4etc\"\n",
    "    }\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        secrets_path (str or Path): path to json file with s3 credentials, defaults to file named 'secrets.json' in the current folder\n",
    "        service_name (str): name of service to use with the client, defaults to s3\n",
    "        endpoint_url (str): the complete URL to use for the constructed client, if provided\n",
    "        region_name (str): the name of the region associated with the client, if provided\n",
    "\n",
    "    Returns:\n",
    "        s3 (boto3.client): service client instance\n",
    "    '''\n",
    "    with open(secrets_path, 'r') as j:\n",
    "        secrets = json.load(j)\n",
    "        \n",
    "    s3 = boto3.client(\n",
    "        service_name=service_name,\n",
    "        endpoint_url=endpoint_url,\n",
    "        region_name=region_name,\n",
    "        aws_access_key_id=secrets['aws_access_key_id'],\n",
    "        aws_secret_access_key=secrets['aws_secret_access_key'],\n",
    "        config=Config(s3={'addressing_style': 'path'})\n",
    "    )\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8f3dc-c283-4e15-a066-e51242dbf981",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting information about files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57e836b2-fda8-4fb3-8bb5-45e59db8ec47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def list_files(s3, bucket_name='test-bucket', prefix=''):\n",
    "    '''\n",
    "    List files in s3 bucket.\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        s3 (boto3.client): s3 client\n",
    "        bucket_name (str): name of s3 bucket to list files from, defaults to 'test-bucket'\n",
    "        prefix (str or Path): path to folder in s3, defaults to empty string to list all files in the bucket\n",
    "\n",
    "    Returns:\n",
    "        files (list[str]): paths for up to 1000 files in the selected folder or the entire bucket\n",
    "    '''\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=str(prefix))\n",
    "    if response.get('Contents'):\n",
    "        files = [prefix['Key'] for prefix in response['Contents']]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db262a62-dccb-4c46-9704-fcbf6cd7d535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def list_all_files(s3, bucket_name='test-bucket', prefix=''):\n",
    "    '''\n",
    "    List all files in s3 bucket.\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        s3 (boto3.client): s3 client\n",
    "        bucket_name (str): name of s3 bucket to list files from, defaults to 'test-bucket'\n",
    "        prefix (str or Path): path to folder in s3, defaults to empty string to list all files in the bucket\n",
    "\n",
    "    Returns:\n",
    "        all_files (list[str]): paths to all files in the selected folder or the entire bucket\n",
    "    '''\n",
    "    \n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    all_files = []\n",
    "    for page in page_iterator:\n",
    "        files = [file['Key'] for file in page['Contents']]\n",
    "        all_files.extend(files)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47df42e-a7c5-4a4b-921f-403918e0d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def get_s3_folder_size(s3, bucket_name='test-bucket', size_type='MB', prefix=''):\n",
    "    '''\n",
    "    Calculates used space by files in s3 folder or entire bucket in either bytes, kilobytes, megabytes or gigabytes.\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        s3 (boto3.client): s3 client\n",
    "        bucket_name (str): name of s3 bucket to list files from, defaults to 'test-bucket'\n",
    "        size_type (str): unit to use for the returned size, defaults to megabytes if the parameter is not specified or received an invalid value\n",
    "            possible values:\n",
    "                'B': return size in bytes\n",
    "                'KB': return size in kilobytes\n",
    "                'MB': return size in megabytes\n",
    "                'GB': return size in gigabytes\n",
    "        prefix (str or Path): path to folder in s3, defaults to empty string to list all files in the bucket\n",
    "\n",
    "    Returns:\n",
    "        total_size (float): size of the folder in selected unit\n",
    "    '''\n",
    "    \n",
    "    valid_size_types = {\n",
    "        \"B\": 1,\n",
    "        \"KB\": 1024,\n",
    "        \"MB\": 1024 * 1024,\n",
    "        \"GB\": 1024 * 1024 * 1024\n",
    "    }\n",
    "    \n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    total_size = 0\n",
    "    for page in page_iterator:\n",
    "        for file in page['Contents']:\n",
    "            total_size += file['Size']\n",
    "    total_size = total_size / valid_size_types[size_type] if size_type in valid_size_types.keys() else valid_size_types['MB']\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38caef1-7dd5-4421-8e27-7199744cba2f",
   "metadata": {},
   "source": [
    "### Uploading and Downloading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7db5b-3664-45c4-89a0-97518b5146a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import botocore\n",
    "\n",
    "def upload_images_no_list(images, s3, s3_upload_path, bucket_name='test-bucket'):\n",
    "    '''\n",
    "    Upload files to s3, skipping files which have already been uploaded.\n",
    "    \n",
    "    This function checks each file individually if it already exists in s3.\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        images (list[str or Path]): list of paths to files for uploading\n",
    "        s3 (boto3.client): s3 client\n",
    "        s3_upload_path (str or Path): path to folder in s3 where files will be uploaded\n",
    "        bucket_name (str): name of s3 bucket to list files from, defaults to 'test-bucket'\n",
    "    '''\n",
    "    \n",
    "    for img in images:\n",
    "        path_in_bucket = str(Path(s3_upload_path) / Path(img).name)\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket_name, Key=path_in_bucket)\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                # the key (file) does not exist\n",
    "                s3.upload_file(Filename=str(img), Bucket=bucket_name, Key=path_in_bucket)                    \n",
    "            else:\n",
    "              # something else has gone wrong\n",
    "              print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d30ad-b113-48d0-bdba-e53d80d6d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import botocore\n",
    "\n",
    "def upload_images_with_list(images, s3, s3_upload_path, bucket_name='test-bucket'):\n",
    "    '''\n",
    "    Upload files to s3, skipping files which have already been uploaded.\n",
    "    \n",
    "    This function first gets the list of all uploaded files in the target folder and then checks if file already exists there before uploading.\n",
    "    Potentially faster than the function without listing files first, but consumes more memory (and may be unstable if there are too many files in the target folder).\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        images (list[str or Path]): list of paths to files for uploading\n",
    "        s3 (boto3.client): s3 client\n",
    "        s3_upload_path (str or Path): path to folder in s3 where files will be uploaded\n",
    "        bucket_name (str): name of s3 bucket to list files from, defaults to 'test-bucket'\n",
    "    '''\n",
    "    \n",
    "    existing_files = []\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=str(s3_upload_path))\n",
    "    if response.get('Contents'):\n",
    "        existing_files = [prefix['Key'] for prefix in response['Contents'] if prefix['Key'] != str(s3_upload_path)]\n",
    "\n",
    "    for img in images:\n",
    "        path_in_bucket = str(Path(base_bucket_path) / Path(img).name)\n",
    "        if path_in_bucket not in existing_files:\n",
    "            s3.upload_file(Filename=str(img), Bucket=BUCKET['Name'], Key=path_in_bucket)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2cc26e3-513c-44fb-97e5-a7d7f66f50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def download_s3_files_in_parallel(download_path, s3_paths,\n",
    "                                  n_processes=os.cpu_count(), n_threads=10, \n",
    "                                  secrets_path='secrets.json', service_name='s3', endpoint_url=None, region_name=None, \n",
    "                                  bucket_name='test-bucket'):\n",
    "    '''\n",
    "    Downloads files from s3 using multiprocessing and multithreading (per process). Skips files which have already been downloaded.\n",
    "    Files are downloaded into the specified folder while preserving the structure of their original paths in s3 (for consistency).\n",
    "    \n",
    "    Includes 3 helper functions:\n",
    "        thread_init: for setting up a connection to s3 on each thread's initialization\n",
    "        download_file: task for downloading a file to be assigned to each thread\n",
    "        process_task: task for creating threads in the current process for downloading images to be assigned to each process\n",
    "\n",
    "    Requires json file with s3 credentials in following format:\n",
    "    {\n",
    "        aws_access_key_id: \"qwerty\"\n",
    "        aws_secret_access_key: \"a1b2c3d4etc\"\n",
    "    }\n",
    "    \n",
    "    -----\n",
    "    Args:\n",
    "        download_path(str or Path): path to folder where files will be downloaded\n",
    "        s3_paths(list[str or Path]): list of paths to files in s3 for downloading\n",
    "        n_processes(int): number of processes to use, defaults to all available CPUs on the machine\n",
    "        n_threads(int): number of threads per process to use, defaults to 10\n",
    "        secrets_path (str or Path): path to json file with s3 credentials, defaults to file named 'secrets.json' in the current folder\n",
    "        service_name (str): name of service to use with the client, defaults to s3\n",
    "        endpoint_url (str): the complete URL to use for the constructed client, if provided\n",
    "        region_name (str): the name of the region associated with the client, if provided\n",
    "        bucket_name (str): name of s3 bucket to list files from, defaults to 'test-bucket'\n",
    "    '''\n",
    "    \n",
    "    def thread_init(local, service_name, endpoint_url, region_name, bucket_name, key, secret):\n",
    "        session = boto3.session.Session()\n",
    "        resource = session.resource(\n",
    "                        service_name=service_name,\n",
    "                        endpoint_url=endpoint_url,\n",
    "                        region_name=region_name,\n",
    "                        aws_access_key_id=key,\n",
    "                        aws_secret_access_key=secret,\n",
    "                        config=Config(s3={'addressing_style': 'path'})\n",
    "                        )\n",
    "        local.bucket = resource.Bucket(bucket_name)\n",
    "    \n",
    "    def download_file(download_path, s3_file, local):\n",
    "        s3_file = Path(s3_file)\n",
    "        download_path = Path(download_path)\n",
    "        local_file = download_path / s3_file\n",
    "        \n",
    "        if Path.is_file(local_file) and local_file.stat().st_size:\n",
    "            return\n",
    "        \n",
    "        local_file.parents[0].mkdir(exist_ok=True, parents=True)\n",
    "        local.bucket.download_file(str(s3_file), str(local_file))\n",
    "\n",
    "    def process_task(download_path, p_files, n_threads, service_name, endpoint_url, region_name, bucket_name, key, secret):\n",
    "        process_local = threading.local()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=n_threads, initializer=thread_init, initargs=(process_local, service_name, endpoint_url, region_name, bucket_name, key, secret)) as executor:\n",
    "            futures = [executor.submit(download_file, download_path, s3_file, process_local) for s3_file in p_files]\n",
    "            for future in tqdm(futures):\n",
    "                future.result()\n",
    "                \n",
    "    with open('secrets.json', 'r') as j:\n",
    "        secrets = json.load(j)\n",
    "    key = secrets['aws_access_key_id']\n",
    "    secret = secrets['aws_secret_access_key']\n",
    "\n",
    "    files_per_process = np.array_split(np.array(s3_paths), n_processes)\n",
    "    files_per_process = [list(x) for x in files_per_process]\n",
    "    gc.collect()\n",
    "                \n",
    "    processes = []\n",
    "    for p_files in files_per_process:\n",
    "        p = mp.Process(target=process_task, args=(download_path, p_files, n_threads, service_name, endpoint_url, region_name, bucket_name, key, secret))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    for p in processes:\n",
    "        p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f87ce0-4e44-4a11-bcff-b5d84f64f5a2",
   "metadata": {},
   "source": [
    "### Usage example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a0e1ad-16ce-410e-9992-ea0875b80187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1189/1189 [01:32<00:00, 12.84it/s]]\n",
      "100%|██████████| 1190/1190 [01:42<00:00, 11.57it/s]]\n",
      "100%|██████████| 1189/1189 [01:42<00:00, 11.58it/s]\n",
      "100%|██████████| 1190/1190 [01:57<00:00, 10.09it/s] \n",
      "100%|██████████| 1190/1190 [02:13<00:00,  8.93it/s] \n",
      "100%|██████████| 1190/1190 [02:14<00:00,  8.84it/s]\n",
      "100%|██████████| 1190/1190 [02:26<00:00,  8.12it/s]\n",
      "100%|██████████| 1189/1189 [02:37<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting a list of test images in s3 and downloading them\n",
    "s3 = setup_s3_connection()\n",
    "files_to_download = list_all_files(s3, prefix='projects/test/data/images/')\n",
    "download_s3_files_in_parallel('test_download', files_to_download)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
