{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95067135-c89a-40df-935b-8a9976547fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cloth-segmentation'...\n",
      "remote: Enumerating objects: 62, done.\u001b[K\n",
      "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
      "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
      "remote: Total 62 (delta 5), reused 55 (delta 2), pack-reused 1\u001b[K\n",
      "Unpacking objects: 100% (62/62), 16.84 MiB | 14.32 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# Cloning GitHub repository for cloth segmentation by levindabhi\n",
    "\n",
    "!git clone https://github.com/levindabhi/cloth-segmentation.git\n",
    "!mv -T cloth-segmentation cloth_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3871fe9c-ec53-44bf-a021-9ee336e71a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from timm) (5.3.1)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/lib/python3/dist-packages (from timm) (0.14.1)\n",
      "Requirement already satisfied: torch>=1.7 in /usr/lib/python3/dist-packages (from timm) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from huggingface-hub->timm) (0.6.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub->timm) (3.0.12)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.8/site-packages (from huggingface-hub->timm) (23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->timm) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.8/site-packages (from requests->huggingface-hub->timm) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->timm) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->timm) (2.8)\n",
      "Installing collected packages: huggingface-hub, timm\n",
      "Successfully installed huggingface-hub-0.14.1 timm-0.6.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: requests[socks] in ./.local/lib/python3.8/site-packages (from gdown) (2.28.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from gdown) (4.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.8/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-4.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch-metric-learning\n",
      "  Downloading pytorch_metric_learning-2.1.1-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from pytorch-metric-learning) (0.22.2.post1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/lib/python3/dist-packages (from pytorch-metric-learning) (1.13.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from pytorch-metric-learning) (4.64.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from pytorch-metric-learning) (1.23.5)\n",
      "Installing collected packages: pytorch-metric-learning\n",
      "Successfully installed pytorch-metric-learning-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.125-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.125\n",
      "  Downloading botocore-1.29.125-py3-none-any.whl (10.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.local/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.125->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.125->boto3) (1.14.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.26.125 botocore-1.29.125 jmespath-1.0.1 s3transfer-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install gdown\n",
    "!pip install pytorch-metric-learning\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec623858-6744-4b4d-bac4-84349fba5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1mhF3yqd7R-Uje092eypktNl-RoZNuiCJ\n",
      "From (redirected): https://drive.google.com/uc?id=1mhF3yqd7R-Uje092eypktNl-RoZNuiCJ&confirm=t&uuid=cca5a4f2-f8e6-4b55-98ed-b0f640803220\n",
      "To: /home/ubuntu/cloth_segmentation/cloth_segm_u2net_latest.pth\n",
      "100%|██████████| 177M/177M [00:01<00:00, 103MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cloth_segmentation/cloth_segm_u2net_latest.pth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# Download pre-trained weights for the segmentation model (they're now gone, RIP)\n",
    "gdown.download(id=\"1mhF3yqd7R-Uje092eypktNl-RoZNuiCJ\", output=\"cloth_segmentation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333b29de-b875-4311-ad86-fb87cb24eaff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import timm\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "from cloth_segmentation.utils.saving_utils import load_checkpoint_mgpu\n",
    "from cloth_segmentation.networks import U2NET\n",
    "\n",
    "import cv2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58362ac0-3164-4661-8536-c8b8a2f02a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = 's3-bucket'\n",
    "\n",
    "# Requires secrets.json file with the S3 connection credentials in the current folder\n",
    "with open('secrets.json', 'r') as j:\n",
    "     secrets = json.load(j)\n",
    "        \n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=secrets['aws_access_key_id'],\n",
    "    aws_secret_access_key=secrets['aws_secret_access_key'],\n",
    "    config=Config(s3={'addressing_style': 'path'})\n",
    ")\n",
    "\n",
    "path_prefix = 'projects/street2shop/data/image_archives/'\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Delimiter = '/', Prefix=path_prefix)\n",
    "shop_zips_s3 = [prefix['Key'] for prefix in response['Contents'] if prefix['Key'] != path_prefix]\n",
    "shop_zips_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb5f89f-8a2d-4972-9e92-46f1631aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transform to add 0 padding to square size for an image\n",
    "\n",
    "class SquarePadTensor:\n",
    "    def __call__(self, image):\n",
    "        _, h, w = image.shape\n",
    "        s = max(w, h)\n",
    "        lft = (s - w) // 2\n",
    "        rgt = s - w - lft\n",
    "        top = (s - h) // 2\n",
    "        bot = s - h - top\n",
    "\n",
    "        padding = (lft, top, rgt, bot)\n",
    "        return transforms.functional.pad(image, padding, 0, 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2666dd80-6913-4f88-b313-a3dba947d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seg = transforms.Compose([\n",
    "        SquarePadTensor(),\n",
    "        transforms.Resize(768),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                             [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "transform_before_masking = transforms.Compose([\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_after_masking = transforms.Compose([\n",
    "        transforms.Resize(224)\n",
    "    ])\n",
    "\n",
    "transforms_emb = [transform_before_masking, \n",
    "                  transform_after_masking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f2bb1e-ad3a-47db-bf6e-16e3ac9e6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatasetSeg(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.images = dataframe[\"image_path\"].values\n",
    "        self.masks = dataframe[\"mask_path\"].values\n",
    "        \n",
    "        self.to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_img = self.images[idx]\n",
    "        path_to_mask = self.masks[idx]\n",
    "        \n",
    "        image = cv2.imread(str(path_to_img).strip())\n",
    "        try:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            print(path_to_img)\n",
    "            raise e\n",
    "            \n",
    "        image = self.to_tensor(image).to(device)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, path_to_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463e66e6-1885-490a-bbdd-9ffd9534c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.img_labels = dataframe[\"label\"].values\n",
    "        self.images = dataframe[\"image_path\"].values\n",
    "        self.masks = dataframe[\"mask_path\"].values\n",
    "        \n",
    "        self.to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "        self.pad_for_masking = transforms.Compose([\n",
    "            SquarePadTensor(),\n",
    "            transforms.Resize(768)\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.img_labels[idx]\n",
    "        \n",
    "        image = cv2.imread(str(self.images[idx]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        mask = cv2.imread(str(self.masks[idx]), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        image = self.to_tensor(image).to(device)\n",
    "        mask = self.to_tensor(mask).to(device)\n",
    "        \n",
    "        if not self.transform:\n",
    "            image = self.pad_for_masking(image)\n",
    "            masked_image = image * mask\n",
    "            return masked_image, label\n",
    "        elif isinstance(self.transform, list) and len(self.transform) == 2:\n",
    "            image = self.transform[0](image)\n",
    "            image = self.pad_for_masking(image)\n",
    "            masked_image = image * mask\n",
    "            masked_image = self.transform[1](masked_image)\n",
    "            return masked_image, label\n",
    "        else:\n",
    "            image = self.pad_for_masking(image)\n",
    "            masked_image = image * mask\n",
    "            masked_image = self.transform(masked_image)\n",
    "            return masked_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f9b61-261d-4775-b681-d11b8ca57122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsPipeline():\n",
    "    def __init__(self, \n",
    "                 s3, \n",
    "                 bucket_name, \n",
    "                 model, \n",
    "                 model_seg, \n",
    "                 transform_emb,\n",
    "                 transform_seg,\n",
    "                 batch_size=256, \n",
    "                 batch_size_seg=8, \n",
    "                 files_root_folder=\"imgs\",\n",
    "                 s3_masked_zips_path=\"street2shop/data/image_archives_masked/\",\n",
    "                 s3_embeddings_path=\"street2shop/embeddings/\"):\n",
    "        self.s3 = s3\n",
    "        self.bucket = bucket_name\n",
    "        self.shop_s3_path = None\n",
    "        self.s3_masked_zips_path = s3_masked_zips_path\n",
    "        self.s3_embeddings_path = s3_embeddings_path\n",
    "        \n",
    "        self.root_path = Path(files_root_folder)\n",
    "        self.root_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.shop_name = Path(shop_s3_path).stem\n",
    "        self.shop_zip_filename = Path(shop_s3_path).name\n",
    "\n",
    "        self.shop_path = self.root_path / self.shop_name\n",
    "        self.zip_path = self.root_path / self.shop_zip_filename\n",
    "\n",
    "        self.shop_imgs_path = self.shop_path / \"images\"\n",
    "        self.shop_masks_path = self.shop_path / \"masks\"\n",
    "        \n",
    "        self.embeddings_path = None\n",
    "        \n",
    "        self.df = None\n",
    "        self.df_filtered = None\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_seg = model_seg\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model_seg.eval()\n",
    "        \n",
    "        self.transform_emb = transform_emb\n",
    "        self.transform_seg = transform_seg\n",
    "        \n",
    "        self.batch_size_seg = batch_size_seg\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_shop_s3_path(self, shop_s3_path):\n",
    "        self.shop_s3_path = shop_s3_path\n",
    "        \n",
    "    def download_and_unpack_zip(self):\n",
    "        if self.shop_s3_path:\n",
    "            self.s3.download_file(self.bucket, str(self.shop_s3_path), str(self.zip_path))\n",
    "            shutil.unpack_archive(self.zip_path, self.shop_imgs_path)\n",
    "            self.zip_path.unlink()\n",
    "        else:\n",
    "            print(\"Path to image archive in S3 is not set\")\n",
    "        \n",
    "    def create_dataframe(self):\n",
    "        df_dict = {\n",
    "            \"label\": [],\n",
    "            \"image_path\": [],\n",
    "            \"mask_path\": []\n",
    "        }\n",
    "\n",
    "        for hash_first_two in self.shop_imgs_path.iterdir():\n",
    "            for hash_second_two in hash_first_two.iterdir():\n",
    "                for img_path in hash_second_two.iterdir():\n",
    "                    if img_path.stem != \".ipynb_checkpoints\" and img_path.stat().st_size != 0:\n",
    "                        df_dict[\"label\"].append(img_path.stem)\n",
    "                        df_dict[\"image_path\"].append(str(img_path))\n",
    "                        df_dict[\"mask_path\"].append(str(img_path).replace(str(self.shop_imgs_path), str(self.shop_masks_path)))\n",
    "\n",
    "        self.df = pd.DataFrame.from_dict(df_dict)\n",
    "    \n",
    "    def create_masks(self):\n",
    "        if not self.df:\n",
    "            return \"Create dataframe first\"\n",
    "        \n",
    "        dataset_seg = ImageDatasetSeg(self.df, transform=self.transform_seg)\n",
    "        dataloader_seg = DataLoader(self.dataset_seg, batch_size=self.batch_size_seg, shuffle=False)\n",
    "\n",
    "        self.model_seg.eval()\n",
    "        for data, path_to_mask in tqdm(dataloader_seg):\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model_seg(data)\n",
    "                output = F.log_softmax(output[0], dim=1)\n",
    "                output = torch.max(output, dim=1, keepdim=True)[1].bool().float()   \n",
    "                output = output.detach().cpu()\n",
    "                for i in range(output.shape[0]):\n",
    "                    Path(path_to_mask[i]).parent.mkdir(parents=True, exist_ok=True)\n",
    "                    to_pil_image(output[i], \"L\").save(path_to_mask[i])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    def upload_masks_to_s3(self):\n",
    "        shutil.make_archive(self.shop_path, 'zip', self.shop_path)  # Makes zipfile {root_path}/{shop_name}.zip from folder {root_path}/{shop_name}\n",
    "        self.s3.upload_file(Filename=str(self.zip_path), Bucket=self.bucket, Key=self.s3_masked_zips_path + self.shop_zip_filename)\n",
    "        self.zip_path.unlink()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __calculate_mask_fillrate(row):\n",
    "        mask = cv2.imread(str(row), cv2.IMREAD_GRAYSCALE)\n",
    "        pixels = mask.shape[0] * mask.shape[1]\n",
    "\n",
    "        fillrate = round(mask.sum() / 255 / pixels, 3)\n",
    "        return fillrate\n",
    "\n",
    "    def filter_df(self):\n",
    "        df_filtered = self.df.copy()\n",
    "        df_filtered[\"mask_fillrate\"] = df_filtered[\"mask_path\"].apply(self.__calculate_mask_fillrate)\n",
    "        df_filtered = df_filtered[df_filtered.mask_fillrate >= 0.01].reset_index(drop=True).copy()\n",
    "        self.df_filtered = df_filtered\n",
    "        \n",
    "    def create_embeddings(self):\n",
    "        if self.df_filtered:\n",
    "            df = self.df_filtered\n",
    "        elif :\n",
    "            print(\"Warning! Filtered dataframe wasn't created, using unfiltered dataframe\")\n",
    "            df = self.df\n",
    "        else:\n",
    "            return \"Create and filter dataframe first\"\n",
    "            \n",
    "        dataset = ImageDataset(df, transform=self.self.transform_emb)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.self.batch_size, shuffle=False)\n",
    "\n",
    "        embeddings = {}\n",
    "\n",
    "        self.model.eval()\n",
    "        for data, label in tqdm(dataloader):\n",
    "            data = data.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(data).detach().cpu().numpy()\n",
    "                for i in range(output.shape[0]):\n",
    "                    embeddings[label[i]] = output[i]\n",
    "\n",
    "        self.embeddings_path = self.root_path / f\"{self.shop_name}_embeddings.pickle\"\n",
    "        with open(self.embeddings_path, \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "            \n",
    "    def upload_embeddings(self):\n",
    "        if self.embeddings_path:\n",
    "            s3.upload_file(Filename=str(self.embeddings_path), Bucket=self.bucket, Key=self.s3_embeddings_path + self.embeddings_path.name)\n",
    "        else:\n",
    "            print(\"Embeddings file wasn't created\")\n",
    "            \n",
    "    def cleanup_images(self):\n",
    "        shutil.rmtree(self.shop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf50fc3-0fca-4bc7-92b5-a9956926e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----checkpoints loaded from path: cloth_segmentation/cloth_segm_u2net_latest.pth----\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"cloth_segmentation/cloth_segm_u2net_latest.pth\"\n",
    "\n",
    "net = U2NET(in_ch=3, out_ch=4)\n",
    "net = load_checkpoint_mgpu(net, checkpoint_path)\n",
    "\n",
    "for param in net.parameters():\n",
    "    net.requires_grad = False\n",
    "\n",
    "net.to(device)\n",
    "net.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b44afc-fd82-4903-9223-9771e8fd6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model_checkpoint_train_epoch_50.pt\"\n",
    "\n",
    "model = timm.create_model(\"ig_resnext101_32x8d\", pretrained=False)\n",
    "model.train()\n",
    "\n",
    "model.fc = nn.Linear(in_features=2048, out_features=512)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d667476-4d86-42ad-8b39-eca78787071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_root = \"imgs\"\n",
    "s3_masked_zips_path = \"projects/street2shop/data/image_archives_masked/\"\n",
    "s3_embeddings_path = \"projects/street2shop/embeddings/\"\n",
    "\n",
    "batch_size_seg = 24\n",
    "batch_size = 512\n",
    "\n",
    "pipeline = EmbeddingsPipeline(s3,\n",
    "                              bucket_name, \n",
    "                              model, \n",
    "                              model_seg, \n",
    "                              transform_emb,\n",
    "                              transform_seg,\n",
    "                              batch_size=batch_size,\n",
    "                              batch_size_seg=batch_size_seg,\n",
    "                              files_root_folder=imgs_root,\n",
    "                              s3_masked_zips_path=s3_masked_zips_path)\n",
    "\n",
    "for shop_zip_s3 in tqdm(shop_zips_s3):\n",
    "    pipeline.set_shop_s3_path(shop_zip_s3)\n",
    "    pipeline.download_and_unpack_zip()\n",
    "    pipeline.create_dataframe()\n",
    "    pipeline.create_masks()\n",
    "    pipeline.upload_masks_to_s3()\n",
    "    pipeline.filter_df()\n",
    "    pipeline.create_embeddings()\n",
    "    pipeline.upload_embeddings()\n",
    "    pipeline.cleanup_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
